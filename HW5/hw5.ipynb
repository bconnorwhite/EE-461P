{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 461P: Data Science Principles</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\">Due: Thursday, November 30th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Ensembles (1+12+2 = 15pts)\n",
    "In this question, we will compare performance of different ensemble methods: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [XGBoost](http://xgboost.readthedocs.io/en/latest/).  Note that you have to install xgboost package in addition to scikit-learn.  You can see installation guides [here](http://xgboost.readthedocs.io/en/latest/build.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Two  datasets are provided for this problem. For **each of the datasets ((X1.csv, y1.csv), (X2.csv, y2.csv))**, do the following:\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42.\n",
    "\n",
    "2. Build a classifier using [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [XGBoost](http://xgboost.readthedocs.io/en/latest/), respectively, and answer the following for each classifier.\n",
    "\n",
    " - Mention any design choices (with reasoning/justification) that you made, e.g. the hyperparameters considered for each classifier.\n",
    " - Report the mean error rate (fraction of incorrect labels) and the confusion matrix on test data. <br>\n",
    " - Report the feature importance and time of execution (training and predicting times).\n",
    "\n",
    "3. Compare the three classifiers for the two different datasets ((X1.csv, y1.csv), (X2.csv, y2.csv)) in terms of the misclassification rate.  What are the characteristics of the dataset and the classifiers that resulted in somewhat different comparative results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def alert():\n",
    "    os.system('say \"your program has finished\"')\n",
    "alert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [pd.read_csv(\"X1.csv\"), pd.read_csv(\"X2.csv\")]\n",
    "y = [pd.read_csv(\"y1.csv\"), pd.read_csv(\"y2.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[],[]]\n",
    "X_test = [[],[]]\n",
    "y_train = [[],[]]\n",
    "y_test = [[],[]]\n",
    "\n",
    "X_train[0], X_test[0], y_train[0], y_test[0] = train_test_split(X[0], y[0], test_size=0.33, random_state=42)\n",
    "X_train[1], X_test[1], y_train[1], y_test[1] = train_test_split(X[1], y[1], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, n):\n",
    "    start = time.time()\n",
    "    model.fit(X_train[n], y_train[n])\n",
    "    return model, (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, n):\n",
    "    start = time.time()\n",
    "    y_preds = model.predict(X_test[n])\n",
    "    return y_preds, (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "        min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,\n",
    "        min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0,\n",
    "        warm_start=False, class_weight=None)\n",
    "gbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100,\n",
    "        subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "        max_depth=3, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0,\n",
    "        max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "xgb = XGBClassifier(learning_rate=0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n",
    "        colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = [\"Random Forest Classifier\", \"Gradient Boosting Classifier\", \"XGBoost\"]\n",
    "models = [rfc, gbc, xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set  1  -  Random Forest Classifier =================\n",
      "Training Time:  0.09690713882446289\n",
      "Feature Importances:  [  1.38209466e-01   2.34387999e-02   1.36209609e-04   1.65436724e-02\n",
      "   4.28460042e-02   0.00000000e+00   3.25945442e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.48585171e-03   5.11528737e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.18759142e-03   3.05401981e-02\n",
      "   0.00000000e+00   0.00000000e+00   1.05927647e-01   0.00000000e+00\n",
      "   0.00000000e+00   2.91087066e-01   1.95915206e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   4.15098390e-02   2.34250303e-02\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "Predict Time:  0.0034189224243164062\n",
      "RFC Percent Correct:  0.500991735537\n",
      "Confusion Matrix:  [[1358  342]\n",
      " [ 184 1416]]\n",
      "\n",
      "Set  1  -  Gradient Boosting Classifier =================\n",
      "Training Time:  3.273698091506958\n",
      "Feature Importances:  [ 0.12359967  0.01328609  0.02539225  0.01317231  0.02086548  0.01020775\n",
      "  0.02188709  0.02318999  0.00467864  0.02026064  0.00854145  0.01458274\n",
      "  0.01009431  0.01383859  0.0102437   0.00695808  0.00499981  0.0078938\n",
      "  0.15815392  0.00844881  0.00779071  0.13244914  0.24443425  0.01840551\n",
      "  0.00890761  0.01281572  0.01109688  0.01609751  0.02319681  0.00451074]\n",
      "Predict Time:  0.0074939727783203125\n",
      "RFC Percent Correct:  0.499972451791\n",
      "Confusion Matrix:  [[1480  220]\n",
      " [ 173 1427]]\n",
      "\n",
      "Set  1  -  XGBoost =================\n",
      "Training Time:  16.748476028442383\n",
      "Feature Importances:  [ 0.06025061  0.02998226  0.02958174  0.0275791   0.02677805  0.03192768\n",
      "  0.02992504  0.02609143  0.02254391  0.02860903  0.02912399  0.02700692\n",
      "  0.0234594   0.0282085   0.02586256  0.0323282   0.02603422  0.02929565\n",
      "  0.07238084  0.02752189  0.02632031  0.07060708  0.07404017  0.02391715\n",
      "  0.02735023  0.02923843  0.02752189  0.0282085   0.03118384  0.02712136]\n",
      "Predict Time:  0.34784603118896484\n",
      "RFC Percent Correct:  0.500045913682\n",
      "Confusion Matrix:  [[1527  173]\n",
      " [ 118 1482]]\n",
      "\n",
      "Set  2  -  Random Forest Classifier =================\n",
      "Training Time:  0.37833404541015625\n",
      "Feature Importances:  [ 0.          0.          0.          0.05571409  0.06400119  0.10556154\n",
      "  0.05310405  0.10294685  0.04726098  0.2009072   0.          0.11332271\n",
      "  0.          0.04161565  0.          0.07828738  0.01965206  0.\n",
      "  0.11762631  0.        ]\n",
      "Predict Time:  0.005563974380493164\n",
      "RFC Percent Correct:  0.502279522498\n",
      "Confusion Matrix:  [[600 254]\n",
      " [118 678]]\n",
      "\n",
      "Set  2  -  Gradient Boosting Classifier =================\n",
      "Training Time:  3.57614803314209\n",
      "Feature Importances:  [ 0.03584561  0.00159719  0.00168504  0.07582648  0.06604653  0.04129662\n",
      "  0.04959452  0.07841117  0.02868397  0.12627242  0.04517815  0.08058069\n",
      "  0.          0.07592878  0.          0.09525197  0.06244134  0.00640996\n",
      "  0.09112298  0.03782659]\n",
      "Predict Time:  0.016571044921875\n",
      "RFC Percent Correct:  0.499978696051\n",
      "Confusion Matrix:  [[762  92]\n",
      " [ 64 732]]\n",
      "\n",
      "Set  2  -  XGBoost =================\n",
      "Training Time:  7.4078309535980225\n",
      "Feature Importances:  [ 0.04821619  0.0289686   0.02585788  0.05667347  0.05881209  0.0608535\n",
      "  0.05365996  0.06017303  0.05570137  0.06211723  0.05521532  0.05472927\n",
      "  0.03577331  0.06124235  0.03032954  0.06493632  0.05385438  0.02751045\n",
      "  0.05949256  0.04588315]\n",
      "Predict Time:  0.06702709197998047\n",
      "RFC Percent Correct:  0.499978696051\n",
      "Confusion Matrix:  [[788  66]\n",
      " [ 38 758]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for set in range(0, 2):\n",
    "    for m in range(0, len(models)):\n",
    "        print(\"Set \", set+1, \" - \", titles[m], \"=================\")\n",
    "        model, elapsed = train(models[m], set)\n",
    "        print(\"Training Time: \", elapsed)\n",
    "        print(\"Feature Importances: \", model.feature_importances_)\n",
    "        y_preds, elapsed = predict(model, set)\n",
    "        print(\"Predict Time: \", elapsed)\n",
    "        print(\"RFC Percent Correct: \", np.mean(np.array(y_test[set]) != np.array(y_preds)))\n",
    "        print(\"Confusion Matrix: \", confusion_matrix(y_test[set], y_preds))\n",
    "        print()\n",
    "#alert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the \"nbasalariesfull.csv\" data set from HMK4, your goal is to build a Bokeh visualization which allows the user to explore how salary (on a log scale) varies with points per game (PSG) and age. You will create a visualization that allows the user to toggle the X axis of a scatter plot between PSG and age, with the y-axis always being log Salary. Also add the hover tool so that if the user hovers over a datapoint in the plot a window pops up that shows the player name, team, position, salary, and the current x variable (PSG or age) depending on the current tab.  Color each point according to a player's position and provide a legend for the colors. Add the ability to Zoom in/out.  Add slight horizontal jitter to a player's age.\n",
    "\n",
    "Hints: \n",
    "1. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for hover and zoom tool examples.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "3. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "4. See: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html  for how to use jitter transform\n",
    "5. Use output_notebook() from Bokeh to output the plot to your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'bokeh.transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-403f63b4cc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPanel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomJS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnDataSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHoverTool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBoxZoomTool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'bokeh.transform'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, BoxZoomTool\n",
    "from bokeh.transform import jitter\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "data = pd.read_csv(\"nbasalariesfull.csv\")\n",
    "data[\"logsalary\"] = data.SALARY.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Clustering (15 pts)\n",
    "## Part a\n",
    "\n",
    "This problem explores scaling, clustering, and one method of evaluating the quality of a clustering. You will also examine some of the issues in k-means clustering, and some ways to mitigate these problems. You will be using Fisher's Iris dataset available in sklearn.\n",
    "\n",
    "We wish to cluster the dataset to find any similar groups of flowers. However, we are not certain that the scales of the various features are well-suited for clustering. We will use scipy's k-means clustering package for this problem.\n",
    "\n",
    "When calculating the cluster purity, use the following equations:\n",
    "\n",
    "$ClassPurity(C_i$) = $\\frac{1}{|C_i|}\\max_j(|C_i|_{class=j})$\n",
    "\n",
    "$NetPurity$ = $\\sum_{i=1}^k\\frac{|C_i|}{|D|}purity(C_i)$\n",
    "\n",
    "where $|C_i|$ is the total number of data points assigned to cluster $C_i$, $|C_i|_{class=j}$ is the number of data points from class $j$ assigned to cluster $C_i$ and $D$ refers to the whole dataset.\n",
    "\n",
    "1. Cluster the data into 4 clusters, using K-means and calculate the NetPurity for your solution.\n",
    "2. Now linearly scale each feature so that the values range from 0 to 1. Cluster the data using the k-means algorithm (as before), and calculate the cluster purity for the clustering. Report the calculations you used to scale the features, as well as the NetPurity.\n",
    "3. Linearly scale the original dataset features so that the distribution of values for each feature has a mean of 0 and a standard deviation of 1. Cluster the data as before, and report the NetPurity obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=4, random_state=0)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b\n",
    "\n",
    "Run both single link and complete link hierarchical clustering algorithms on the iris dataset, and submit the two dendograms. \n",
    "\n",
    "1. For each dendogram, visually inspect it to suggest what value(s) of $k$ (between 1 and 6) seem reasonable to choose for this dataset. \n",
    "2. Do you observe any difference in the structure of the 2 dendograms? How do you explain this difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c\n",
    "\n",
    "1. What is the cost function minimised in the k-means clustering algorithm?\n",
    "2. Explain how this can be generalized to be of the form $J = \\sum_{i=1}^k n_i s_i$, where $s_i$ is the average squared (Euclidean) distance between pairs of points in cluster $i$ (every point is compared with every point - including itself - in the same cluster, so we have $n_i^2$ comparisions that are averaged over to get $s_i$), \n",
    "and $n_i$ is the total number of points in cluster $i$.\n",
    "3. What would the benefit of such a generalized expression be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Standard Expression, $J = \\sum_{i=1}^k \\sum_{x \\in S_i} ||x-\\mu_i||^2$, where \\mu_i is the mean of the points in S_i\n",
    "2. Generalized expression, \n",
    "    $J = \\sum_{i=1}^k n_i * (||x-\\mu_i||^2 /n_i)$ where n_i is the number of samples in each cluster\n",
    "    \n",
    "    $J = \\sum_{i=1}^k n_i * s_i $ where s_i is the variance within each cluster\n",
    "\n",
    "3. Benefit: $s_i$ can now be replaced with any similarity metric based on the clustering problem requirements"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
